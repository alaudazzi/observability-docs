[[monitor-aws-waf-firehose]]
= Monitor Web Application Firewall (WAF) logs

++++
<titleabbrev>Monitor WAF logs</titleabbrev>
++++

In this section, you'll learn how to send AWS WAF events from AWS to your Elastic stack using Amazon Data Firehose.

You will go through the following steps:

- Select a WAF-compatible resource (CloudFront distribution)
- Create a delivery stream in Amazon Data Firehose
- Create a web Access Control List (ACL) to generate WAF logs
- Set up logging to forward the logs to the Elastic stack using a Firehose stream
- Visualize your WAF logs in {kib}

[discrete]
[[firehose-cloudtrail-prerequisites]]
== Before you begin

We assume that you already have:

- An AWS account with permissions to pull the necessary data from AWS.
- A deployment using our hosted {ess} on {ess-trial}[{ecloud}]. The deployment includes an {es} cluster for storing and searching your data, and {kib} for visualizing and managing your data. AWS Data Firehose works with Elastic Stack version 7.17 or greater, running on Elastic Cloud only.

IMPORTANT: Make sure the deployment is on AWS, because the Amazon Data Firehose delivery stream connects specifically to an endpoint that needs to be on AWS.

[discrete]
[[firehose-waf-step-one]]
== Step 1: Install AWS integration in {kib}

. Install AWS integrations to load index templates, ingest pipelines, and dashboards into {kib}. In {kib}, navigate to *Management* > *Integrations* and find the AWS Integration by browsing the catalog.

. Navigate to the *Settings* tab and click *Install AWS assets*. Confirm by clicking *Install AWS* in the popup.

. Install AWS Firehose integration assets in Kibana. 

NOTE: Amazon Data Firehose integration is currently in beta. Make sure to enable *Display beta integrations*.

[discrete]
[[firehose-waf-resource-step-two]]
== Step 2: Select a WAF-compatible resource

You can create and attach a web ACL to several AWS resources:

- CloudFront distribution
- Application Load Balancers
- Amazon API Gateway REST APIs
- Amazon App Runner services
- AWS AppSync GraphQL APIs
- Amazon Cognito user pools
- AWS Verified Access Instances

If you don't have an existing resource for testing, you can create a simple CloudFront distribution.

[discrete]
[[firehose-waf-step-three]]
== Step 3: Create a delivery stream in Amazon Data Firehose

. Go to the https://console.aws.amazon.com/[AWS console] and navigate to Amazon Data Firehose.  

. Click *Create Firehose stream* and choose the source and destination of your Firehose stream. Unless you are streaming data from Kinesis Data Streams, set source to `Direct PUT` and destination to `Elastic`. 

. Provide a meaningful *Firehose stream name* that will allow you to identify this delivery stream later.

NOTE: For advanced use cases, source records can be transformed by invoking a custom Lambda function. When using Elastic integrations, this should not be required.

[discrete]
[[firehose-waf-step-four]]
== Step 4: Specify the destination settings for your Firehose stream

. From the *Destination settings* panel, specify the following settings:
+
* *Elasticsearch endpoint URL*: Enter the Elasticsearch endpoint URL of your Elasticsearch cluster. To find the Elasticsearch endpoint, go to the Elastic Cloud console and select *Connection details*. Here is an example of how it looks like: `https://my-deployment.es.us-east-1.aws.elastic-cloud.com`.
+
* *API key*: Enter the encoded Elastic API key. To create an API key, go to the Elastic Cloud console, select *Connection details* and click *Create and manage API keys*. If you are using an API key with *Restrict privileges*, make sure to review the Indices privileges to provide at least "auto_configure" & "write" permissions for the indices you will be using with this delivery stream. 
+
* *Content encoding*: For a better network efficiency, leave content encoding set to GZIP. 
+
* *Retry duration*: Determines how long Firehose continues retrying the request in the event of an error. A duration of 60-300s should be suitable for most use cases.
+
* *Parameters*:
+
  ** `es_datastream_name`: Your Firehose name must start with the prefix aws-waf-logs- or it will not show up later.
  ** `include_cw_extracted_fields`: This parameter is optional and can be set when using a CloudWatch logs subscription filter as the Firehose data source. When set to true, extracted fields generated by the filter pattern in the subscription filter will be collected. Setting this parameter can add many fields into each record and may significantly increase data volume in Elasticsearch. As such, use of this parameter should be carefully considered and used only when the extracted fields are required for specific filtering and/or aggregation.

[discrete]
[[firehose-waf-step-five]]
== Step 5: Create a web ACL

To create a new web ACL, follow these steps:

. Go to the https://console.aws.amazon.com/[AWS console] and navigate to the *WAF & Shield* page. 

. Describe web ACL and associate it to an AWS resource by providing the following information:
+
- Resource type
- Name
- Associated AWS resource

. Add rules and rule group.
+
Add a 1-2 rules in the "Free rule groups" list from the AWS managed rule groups. Keep all other settings to their default values.

. Set the rule priority.
+
Keep default values.

. Configure metrics.
+
Keep default values.

. Review and create the web ACL.
+
Review and confim your previous settings to create the web ACL.

[discrete]
[[firehose-waf-step-six]]
== Step 6: Set up logging

. Go to the web ACL you created in the previous step.

. Open the *Logging and metrics* section and edit the following settings:
+
- *Logging destination*: select "Amazon Data Firehose stream"
- *Amazon Data Firehose stream*: select the Firehose stream you created in step 3.

WAF creates the required IAM role.
If your Firehose stream name doesn't appear in the list, make sure the name you chose for the stream starts with `aws-waf-logs-`, as prescribed by AWS naming conventions.

[discrete]
[[firehose-waf-step-seven]]
== Step 7: Visualize your WAF logs in {kib}

You can now log into your Elastic stack to check if the WAF logs are flowing. To generate logs, you can use cURL to send HTTP requests to our testing CloudFront distribution.

[source,console]
----
curl -i https://<your cloudfront distribution>.cloudfront.net
----

To maintain a steady flow of logs, you can use `watch -n 5 to repeat the command every 5 seconds.

[source,console]
----
watch -n 5 curl -i https://<your cloudfront distribution>.cloudfront.net
----

Navigate to Kibana and visualize the first WAF logs in your Elastic stack.

[role="screenshot"]
image::firehose-waf-logs.png[Firehose WAF logs in Kibana]